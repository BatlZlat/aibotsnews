#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ –ø–ª–∞–≥–∏–∞—Ç –∏ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
"""

import re
import hashlib
import difflib
from collections import Counter
import requests
from bs4 import BeautifulSoup
import time
import json

class UniquenessChecker:
    def __init__(self):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≤–µ—Ä—â–∏–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
        """
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def calculate_text_similarity(self, text1, text2):
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–≤—É—Ö —Ç–µ–∫—Å—Ç–æ–≤
        
        Args:
            text1 (str): –ü–µ—Ä–≤—ã–π —Ç–µ–∫—Å—Ç
            text2 (str): –í—Ç–æ—Ä–æ–π —Ç–µ–∫—Å—Ç
            
        Returns:
            float: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ (0-1)
        """
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç—ã
        text1_clean = re.sub(r'[^\w\s]', '', text1.lower())
        text2_clean = re.sub(r'[^\w\s]', '', text2.lower())
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º SequenceMatcher –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ö–æ–∂–µ—Å—Ç–∏
        similarity = difflib.SequenceMatcher(None, text1_clean, text2_clean).ratio()
        
        return similarity
    
    def extract_key_phrases(self, text, min_length=3, max_length=6):
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑ –∏–∑ —Ç–µ–∫—Å—Ç–∞
        
        Args:
            text (str): –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            min_length (int): –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ñ—Ä–∞–∑—ã
            max_length (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ñ—Ä–∞–∑—ã
            
        Returns:
            list: –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑
        """
        words = re.findall(r'\b\w+\b', text.lower())
        phrases = []
        
        for length in range(min_length, max_length + 1):
            for i in range(len(words) - length + 1):
                phrase = ' '.join(words[i:i + length])
                if len(phrase) > 10:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ñ—Ä–∞–∑—ã
                    phrases.append(phrase)
        
        return phrases
    
    def check_phrase_uniqueness(self, text, reference_texts):
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ —Ñ—Ä–∞–∑ –≤ —Ç–µ–∫—Å—Ç–µ
        
        Args:
            text (str): –ü—Ä–æ–≤–µ—Ä—è–µ–º—ã–π —Ç–µ–∫—Å—Ç
            reference_texts (list): –°–ø–∏—Å–æ–∫ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
            
        Returns:
            dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏
        """
        text_phrases = self.extract_key_phrases(text)
        total_phrases = len(text_phrases)
        unique_phrases = 0
        duplicate_phrases = []
        
        for phrase in text_phrases:
            is_unique = True
            for ref_text in reference_texts:
                if phrase in ref_text.lower():
                    is_unique = False
                    duplicate_phrases.append(phrase)
                    break
            
            if is_unique:
                unique_phrases += 1
        
        uniqueness_score = (unique_phrases / total_phrases) * 100 if total_phrases > 0 else 100
        
        return {
            'total_phrases': total_phrases,
            'unique_phrases': unique_phrases,
            'duplicate_phrases': duplicate_phrases,
            'uniqueness_score': uniqueness_score
        }
    
    def check_word_frequency(self, text):
        """
        –ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ
        
        Args:
            text (str): –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            dict: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–ª–æ–≤–∞–º
        """
        words = re.findall(r'\b\w+\b', text.lower())
        word_freq = Counter(words)
        
        # –ù–∞—Ö–æ–¥–∏–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–ª–æ–≤–∞
        repeated_words = {word: count for word, count in word_freq.items() if count > 3}
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ª–µ–∫—Å–∏–∫–∏
        unique_words = len(word_freq)
        total_words = len(words)
        vocabulary_diversity = (unique_words / total_words) * 100 if total_words > 0 else 0
        
        return {
            'total_words': total_words,
            'unique_words': unique_words,
            'vocabulary_diversity': vocabulary_diversity,
            'repeated_words': repeated_words
        }
    
    def check_sentence_structure(self, text):
        """
        –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        
        Args:
            text (str): –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            dict: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
        """
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        sentence_lengths = [len(s.split()) for s in sentences]
        avg_sentence_length = sum(sentence_lengths) / len(sentence_lengths) if sentence_lengths else 0
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –¥–ª–∏–Ω –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        length_variety = len(set(sentence_lengths)) / len(sentence_lengths) * 100 if sentence_lengths else 0
        
        return {
            'total_sentences': len(sentences),
            'avg_sentence_length': avg_sentence_length,
            'length_variety': length_variety,
            'sentence_lengths': sentence_lengths
        }
    
    def check_online_sources(self, text, max_sources=5):
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –æ–Ω–ª–∞–π–Ω –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        
        Args:
            text (str): –¢–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            max_sources (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            
        Returns:
            dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏
        """
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
        key_phrases = self.extract_key_phrases(text, min_length=4, max_length=5)
        
        if not key_phrases:
            return {'online_check': False, 'message': '–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã'}
        
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ—Ä–∞–∑ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        search_phrases = key_phrases[:3]
        
        results = []
        for phrase in search_phrases:
            try:
                # –ò—â–µ–º –≤ Google (—Å–∏–º—É–ª—è—Ü–∏—è)
                search_query = f'"{phrase}"'
                print(f"–ü—Ä–æ–≤–µ—Ä—è—é —Ñ—Ä–∞–∑—É: {phrase}")
                
                # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Ä–µ–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ API
                # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–∞–≥–ª—É—à–∫—É
                results.append({
                    'phrase': phrase,
                    'found_sources': 0,
                    'similarity': 0.0
                })
                
                time.sleep(1)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Ñ—Ä–∞–∑—ã {phrase}: {e}")
        
        return {
            'online_check': True,
            'checked_phrases': len(search_phrases),
            'results': results
        }
    
    def comprehensive_check(self, text, reference_texts=None):
        """
        –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞
        
        Args:
            text (str): –¢–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            reference_texts (list): –°–ø–∏—Å–æ–∫ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
            
        Returns:
            dict: –ü–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏
        """
        print("üîç –ù–∞—á–∏–Ω–∞—é –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏...")
        
        # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        text_hash = hashlib.md5(text.encode()).hexdigest()
        text_length = len(text)
        
        # –ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤
        word_stats = self.check_word_frequency(text)
        print(f"‚úì –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–≤: {word_stats['vocabulary_diversity']:.1f}% —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –ª–µ–∫—Å–∏–∫–∏")
        
        # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        sentence_stats = self.check_sentence_structure(text)
        print(f"‚úì –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {sentence_stats['length_variety']:.1f}% —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ —Ñ—Ä–∞–∑
        if reference_texts:
            phrase_stats = self.check_phrase_uniqueness(text, reference_texts)
            print(f"‚úì –ê–Ω–∞–ª–∏–∑ —Ñ—Ä–∞–∑: {phrase_stats['uniqueness_score']:.1f}% —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ —Ñ—Ä–∞–∑")
        else:
            phrase_stats = {'uniqueness_score': 100, 'duplicate_phrases': []}
        
        # –û–±—â–∏–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
        overall_uniqueness = (
            word_stats['vocabulary_diversity'] * 0.4 +
            sentence_stats['length_variety'] * 0.2 +
            phrase_stats['uniqueness_score'] * 0.4
        )
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        if overall_uniqueness >= 85:
            quality_grade = "–û—Ç–ª–∏—á–Ω–æ"
        elif overall_uniqueness >= 70:
            quality_grade = "–•–æ—Ä–æ—à–æ"
        elif overall_uniqueness >= 50:
            quality_grade = "–£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ"
        else:
            quality_grade = "–¢—Ä–µ–±—É–µ—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏"
        
        results = {
            'text_hash': text_hash,
            'text_length': text_length,
            'overall_uniqueness': overall_uniqueness,
            'quality_grade': quality_grade,
            'word_stats': word_stats,
            'sentence_stats': sentence_stats,
            'phrase_stats': phrase_stats,
            'recommendations': self.generate_recommendations(overall_uniqueness, word_stats, sentence_stats)
        }
        
        print(f"‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {overall_uniqueness:.1f}% —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ ({quality_grade})")
        
        return results
    
    def generate_recommendations(self, uniqueness_score, word_stats, sentence_stats):
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
        
        Args:
            uniqueness_score (float): –û–±—â–∏–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
            word_stats (dict): –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–ª–æ–≤–∞–º
            sentence_stats (dict): –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
            
        Returns:
            list: –°–ø–∏—Å–æ–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        """
        recommendations = []
        
        if uniqueness_score < 70:
            recommendations.append("‚ö†Ô∏è –ù–∏–∑–∫–∞—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–µ—Ä–µ–ø–∏—Å–∞—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ –∫–µ–π—Å–æ–≤.")
        
        if word_stats['vocabulary_diversity'] < 60:
            recommendations.append("üìù –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä–Ω—ã–π –∑–∞–ø–∞—Å. –î–æ–±–∞–≤—å—Ç–µ —Å–∏–Ω–æ–Ω–∏–º—ã –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑—å—Ç–µ –ª–µ–∫—Å–∏–∫—É.")
        
        if sentence_stats['length_variety'] < 50:
            recommendations.append("üìè –û–¥–Ω–æ–æ–±—Ä–∞–∑–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π. –ß–µ—Ä–µ–¥—É–π—Ç–µ –¥–ª–∏–Ω–Ω—ã–µ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.")
        
        if word_stats['repeated_words']:
            top_repeated = sorted(word_stats['repeated_words'].items(), key=lambda x: x[1], reverse=True)[:3]
            recommendations.append(f"üîÑ –ß–∞—Å—Ç–æ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–ª–æ–≤–∞: {', '.join([word for word, count in top_repeated])}. –ó–∞–º–µ–Ω–∏—Ç–µ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏.")
        
        if not recommendations:
            recommendations.append("‚úÖ –ö–æ–Ω—Ç–µ–Ω—Ç –∏–º–µ–µ—Ç —Ö–æ—Ä–æ—à—É—é —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å. –ú–æ–∂–Ω–æ –ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å.")
        
        return recommendations

def main():
    """
    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø—Ä–æ–≤–µ—Ä—â–∏–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
    """
    checker = UniquenessChecker()
    
    # –¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç
    test_text = """
    –ò–ò –±–æ—Ç—ã —Å—Ç–∞–ª–∏ –Ω–µ–∑–∞–º–µ–Ω–∏–º—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º –º–∏—Ä–µ. –û–Ω–∏ –ø–æ–º–æ–≥–∞—é—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ—Ü–µ—Å—Å—ã, 
    —É–ª—É—á—à–∏—Ç—å –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–æ–≤ –∏ –ø–æ–≤—ã—Å–∏—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç—ã. –í –†–æ—Å—Å–∏–∏ –ø–æ–ø—É–ª—è—Ä–Ω—ã —Ç–∞–∫–∏–µ –ò–ò –±–æ—Ç—ã –∫–∞–∫ 
    Yandex.Alice, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–æ–ª–µ–µ —á–µ–º 50 –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.
    
    –†–æ—Å—Å–∏–π—Å–∫–∏–µ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–æ–∑–¥–∞—é—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ò–ò —Ä–µ—à–µ–Ω–∏—è, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–¥ –º–µ—Å—Ç–Ω—ã–π —Ä—ã–Ω–æ–∫ –∏ –∫—É–ª—å—Ç—É—Ä—É. 
    –ü–æ –¥–∞–Ω–Ω—ã–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –†–ê–≠–ö, 67% —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π –ø–ª–∞–Ω–∏—Ä—É—é—Ç –≤–Ω–µ–¥—Ä–∏—Ç—å –ò–ò –±–æ—Ç–æ–≤ –≤ –±–ª–∏–∂–∞–π—à–∏–µ 2 –≥–æ–¥–∞.
    
    –ü—Ä–∏ –≤—ã–±–æ—Ä–µ –ò–ò –±–æ—Ç–∞ –¥–ª—è —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ –±–∏–∑–Ω–µ—Å–∞ —É—á–∏—Ç—ã–≤–∞–π—Ç–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –º–µ—Å—Ç–Ω–æ–≥–æ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –æ 
    –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –†–æ—Å—Å–∏–π—Å–∫–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ —á–∞—Å—Ç–æ –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞—é—Ç –ò–ò –±–æ—Ç–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ 
    –∏ –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –º–µ—Å—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
    """
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å
    results = checker.comprehensive_check(test_text)
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–†–û–í–ï–†–ö–ò:")
    print(f"–û–±—â–∞—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å: {results['overall_uniqueness']:.1f}%")
    print(f"–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {results['quality_grade']}")
    print(f"–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {results['text_length']} —Å–∏–º–≤–æ–ª–æ–≤")
    
    print("\nüìà –î–ï–¢–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"–°–ª–æ–≤–∞—Ä–Ω–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ: {results['word_stats']['vocabulary_diversity']:.1f}%")
    print(f"–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {results['sentence_stats']['length_variety']:.1f}%")
    print(f"–£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å —Ñ—Ä–∞–∑: {results['phrase_stats']['uniqueness_score']:.1f}%")
    
    print("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
    for rec in results['recommendations']:
        print(f"‚Ä¢ {rec}")

if __name__ == "__main__":
    main() 