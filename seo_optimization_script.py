#!/usr/bin/env python3
"""
SEO Optimization Script for AI Bots Content
–î–æ–±–∞–≤–ª—è–µ—Ç —Å–µ—Ä—ã–µ —Å—Ö–µ–º—ã, —Å–∫—Ä—ã—Ç—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ SEO-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é
"""

import os
import re
import glob
from pathlib import Path

class SEOOptimizer:
    def __init__(self):
        self.seo_keywords = {
            'ai_bots': [
                '–ò–ò –±–æ—Ç', '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç', 'AI –ø–æ–º–æ—â–Ω–∏–∫', '—á–∞—Ç –±–æ—Ç',
                '–Ω–µ–π—Ä–æ—Å–µ—Ç—å', '–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ', '–≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ',
                '–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è', '–ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—å', '—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å'
            ],
            'tools': [
                'ChatGPT', 'Claude', 'Google Gemini', 'Microsoft Copilot',
                'Jasper', 'Notion AI', 'Grammarly AI', 'GitHub Copilot',
                'Midjourney', 'Perplexity AI', 'Google Bard'
            ],
            'use_cases': [
                '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ', '—Å–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞', '–∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö',
                '–º–∞—Ä–∫–µ—Ç–∏–Ω–≥', '–¥–∏–∑–∞–π–Ω', '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è', '–æ–±—É—á–µ–Ω–∏–µ',
                '–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –∑–∞–¥–∞—á', '–ø–æ–≤—ã—à–µ–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏'
            ],
            'benefits': [
                '—ç–∫–æ–Ω–æ–º–∏—è –≤—Ä–µ–º–µ–Ω–∏', '—É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞', '–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è',
                'ROI', '—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã', '–∫–µ–π—Å—ã', '–ø—Ä–∏–º–µ—Ä—ã'
            ]
        }
        
    def add_gray_schemas(self, content):
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Å–µ—Ä—ã–µ —Å—Ö–µ–º—ã –∏ —Å–∫—Ä—ã—Ç—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞"""
        
        # –°–æ–∑–¥–∞–µ–º —Å–∫—Ä—ã—Ç—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –∫–æ–Ω—Ü–µ —Å—Ç–∞—Ç—å–∏
        hidden_keywords = self._generate_hidden_keywords()
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–µ—Ä—ã–µ —Å—Ö–µ–º—ã –≤ –Ω–∞—á–∞–ª–æ
        gray_schemas = self._generate_gray_schemas()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫—É
        microdata = self._generate_microdata()
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        optimized_content = f"""{gray_schemas}

{content}

{microdata}

{hidden_keywords}"""
        
        return optimized_content
    
    def _generate_hidden_keywords(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∫—Ä—ã—Ç—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –∫–æ–Ω—Ü–µ —Å—Ç–∞—Ç—å–∏"""
        all_keywords = []
        for category, keywords in self.seo_keywords.items():
            all_keywords.extend(keywords)
        
        # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏
        import random
        random.shuffle(all_keywords)
        
        hidden_section = f"""
<!-- SEO Keywords: {', '.join(all_keywords[:20])} -->
"""
        return hidden_section
    
    def _generate_gray_schemas(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–µ—Ä—ã–µ —Å—Ö–µ–º—ã –¥–ª—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º"""
        schemas = """
<!-- Schema.org structured data -->
<script type="application/ld+json">
{{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "AI Tools Guide 2025",
  "description": "Complete guide to AI tools and bots in 2025",
  "author": {{
    "@type": "Organization",
    "name": "AI Bots Guide"
  }},
  "publisher": {{
    "@type": "Organization",
    "name": "AI Bots Guide",
    "logo": {{
      "@type": "ImageObject",
      "url": "https://aibotsguide.com/logo.png"
    }}
  }},
  "datePublished": "2025-02-01",
  "dateModified": "2025-02-01",
  "mainEntityOfPage": {{
    "@type": "WebPage",
    "@id": "https://aibotsguide.com"
  }}
}}
</script>

<!-- Open Graph meta tags -->
<meta property="og:title" content="AI Tools Guide 2025" />
<meta property="og:description" content="Complete guide to AI tools and bots in 2025" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://aibotsguide.com" />
<meta property="og:image" content="https://aibotsguide.com/og-image.png" />

<!-- Twitter Card meta tags -->
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="AI Tools Guide 2025" />
<meta name="twitter:description" content="Complete guide to AI tools and bots in 2025" />
<meta name="twitter:image" content="https://aibotsguide.com/twitter-image.png" />

<!-- Additional SEO meta tags -->
<meta name="keywords" content="AI tools, artificial intelligence, chatbots, productivity, automation" />
<meta name="robots" content="index, follow" />
<meta name="author" content="AI Bots Guide" />
<meta name="language" content="Russian" />
<meta name="revisit-after" content="7 days" />
"""
        return schemas
    
    def _generate_microdata(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫—É –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        microdata = """
<!-- FAQ Schema -->
<script type="application/ld+json">
{{
  "@context": "https://schema.org",
  "@type": "FAQPage",
  "mainEntity": [
    {{
      "@type": "Question",
      "name": "What are the best AI tools in 2025?",
      "acceptedAnswer": {{
        "@type": "Answer",
        "text": "The best AI tools in 2025 include ChatGPT, Claude, Google Gemini, and Microsoft Copilot."
      }}
    }},
    {{
      "@type": "Question", 
      "name": "How to use AI tools effectively?",
      "acceptedAnswer": {{
        "@type": "Answer",
        "text": "To use AI tools effectively, create clear prompts, iterate on results, and integrate with your workflow."
      }}
    }}
  ]
}}
</script>

<!-- Breadcrumb Schema -->
<script type="application/ld+json">
{{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {{
      "@type": "ListItem",
      "position": 1,
      "name": "Home",
      "item": "https://aibotsguide.com"
    }},
    {{
      "@type": "ListItem", 
      "position": 2,
      "name": "AI Tools",
      "item": "https://aibotsguide.com/tools"
    }},
    {{
      "@type": "ListItem",
      "position": 3, 
      "name": "Guides",
      "item": "https://aibotsguide.com/guides"
    }}
  ]
}}
</script>
"""
        return microdata
    
    def optimize_file(self, file_path):
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –ª–∏ —É–∂–µ —Ñ–∞–π–ª
            if '<!-- SEO Keywords:' in content:
                print(f"–§–∞–π–ª {file_path} —É–∂–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                return
            
            # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
            optimized_content = self.add_gray_schemas(content)
            
            # –°–æ–∑–¥–∞–µ–º SEO –≤–µ—Ä—Å–∏—é —Ñ–∞–π–ª–∞
            seo_file_path = file_path.replace('.md', '-seo.md')
            
            with open(seo_file_path, 'w', encoding='utf-8') as f:
                f.write(optimized_content)
            
            print(f"‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω —Ñ–∞–π–ª: {seo_file_path}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ {file_path}: {e}")
    
    def optimize_all_files(self, content_dir="content"):
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
        print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º SEO-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤...")
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ markdown —Ñ–∞–π–ª—ã
        md_files = []
        for root, dirs, files in os.walk(content_dir):
            for file in files:
                if file.endswith('.md') and not file.endswith('-seo.md'):
                    md_files.append(os.path.join(root, file))
        
        print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(md_files)} —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏")
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Ñ–∞–π–ª
        for file_path in md_files:
            self.optimize_file(file_path)
        
        print(f"‚úÖ SEO-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(md_files)} —Ñ–∞–π–ª–æ–≤")
    
    def create_sitemap(self, content_dir="content"):
        """–°–æ–∑–¥–∞–µ—Ç sitemap.xml –¥–ª—è –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π"""
        print("üó∫Ô∏è –°–æ–∑–¥–∞–µ–º sitemap.xml...")
        
        md_files = []
        for root, dirs, files in os.walk(content_dir):
            for file in files:
                if file.endswith('-seo.md'):
                    md_files.append(os.path.join(root, file))
        
        sitemap_content = """<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
"""
        
        for file_path in md_files:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—É—Ç—å —Ñ–∞–π–ª–∞ –≤ URL
            url_path = file_path.replace('content/', '').replace('-seo.md', '.html')
            url = f"https://aibotsguide.com/{url_path}"
            
            sitemap_content += f"""  <url>
    <loc>{url}</loc>
    <lastmod>2025-02-01</lastmod>
    <changefreq>weekly</changefreq>
    <priority>0.8</priority>
  </url>
"""
        
        sitemap_content += """</urlset>"""
        
        with open('public/sitemap.xml', 'w', encoding='utf-8') as f:
            f.write(sitemap_content)
        
        print("‚úÖ Sitemap.xml —Å–æ–∑–¥–∞–Ω!")
    
    def create_robots_txt(self):
        """–°–æ–∑–¥–∞–µ—Ç robots.txt —Ñ–∞–π–ª"""
        robots_content = """User-agent: *
Allow: /

# Sitemap
Sitemap: https://aibotsguide.com/sitemap.xml

# Disallow admin areas
Disallow: /admin/
Disallow: /private/

# Allow important directories
Allow: /content/
Allow: /public/
"""
        
        with open('public/robots.txt', 'w', encoding='utf-8') as f:
            f.write(robots_content)
        
        print("‚úÖ Robots.txt —Å–æ–∑–¥–∞–Ω!")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    optimizer = SEOOptimizer()
    
    # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã
    optimizer.optimize_all_files()
    
    # –°–æ–∑–¥–∞–µ–º sitemap
    optimizer.create_sitemap()
    
    # –°–æ–∑–¥–∞–µ–º robots.txt
    optimizer.create_robots_txt()
    
    print("\nüéâ SEO-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print("- –î–æ–±–∞–≤–ª–µ–Ω—ã —Å–µ—Ä—ã–µ —Å—Ö–µ–º—ã –∏ –º–∏–∫—Ä–æ—Ä–∞–∑–º–µ—Ç–∫–∞")
    print("- –°–æ–∑–¥–∞–Ω—ã —Å–∫—Ä—ã—Ç—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞")
    print("- –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –≤—Å–µ —Å—Ç–∞—Ç—å–∏")
    print("- –°–æ–∑–¥–∞–Ω sitemap.xml")
    print("- –°–æ–∑–¥–∞–Ω robots.txt")

if __name__ == "__main__":
    main() 